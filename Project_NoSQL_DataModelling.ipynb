{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I. Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python Packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepath to process all the original csv event files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\Desktop\\Preperation2020\\Udacity - Data Engineer\\Data Modelling with Apache Cassandra\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "file_path_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Cassandra tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating an empty list of rows that will have all the data \n",
    "\n",
    "full_data_rows_list = []\n",
    "\n",
    "# for every filepath in the file path list\n",
    "for f in file_path_list:\n",
    "    with open(f,'r',encoding='utf8',newline='') as csvfile:\n",
    "        # creating a csv reader object which is an iterator of lines\n",
    "        csvreader=csv.reader(csvfile)\n",
    "        next(csvreader) # reads first line i.e. header so that below code doesn't read the header everytime.\n",
    "        \n",
    "        # Extracting each row and append to the full_data_rows_list\n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line)\n",
    "            \n",
    "# print the total number of rows identified above\n",
    "# print(len(full_data_rows_list)) \n",
    "# total 8056 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using above extracted rows to create a full event data csv file called event_datafile_full.csv \n",
    "##### It will be used to insert data into the *Apache Cassandra* tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('even_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist', 'firstName', 'gender', 'ItemInSession', 'lastName', 'length',\\\n",
    "                    'level', 'location', 'sessionId', 'song', 'userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow([row[0], row[2], row[3], row[4], row[5], row[6],\\\n",
    "                       row[7], row[8], row[12], row[13], row[16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the total number of rows added in the csv file\n",
    "with open('even_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))\n",
    "# blank rows could not be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Organize the tables in Apache Cassandra to complete the loading and query based analysis of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### event_datafile_new.csv contains all the data required to load into table which contains following columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- artist\n",
    "- firstName of user\n",
    "- gender\n",
    "- item numbers in a session\n",
    "- last name \n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below is a screenshot of what the denormalize data should appear like in the <font color=red>event_datafile_new.csv</font> after above code is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![even_data_file_new.csv]('C:\\Users\\Rishabh\\Desktop\\Preperation2020\\Udacity - Data Engineer\\Data Modelling with Apache Cassandra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a cluster and connecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes a connection to a cassandra instance on the local host i.e. \n",
    "# 127.0.0.1\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "try:\n",
    "    cluster = Cluster(['127.0.0.1'])\n",
    "    \n",
    "    # To establish connection and begin executing queries, a session is setup.\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a KeySpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1188bcfecf8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS sparkify\n",
    "WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor' : 1}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set this keyspace for creating tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('sparkify')\n",
    "# session.use('sparkify')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries to be analyze from the database are:\n",
    "* Give me the artist, song title and song's length in the music app history that was heard during sessionId = 338, and itemInSession = 4\n",
    "* Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "* Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query No. (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give me the artist, song title and song's length in the music app history that was heard during sessionId = 338, and itemInSession = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question we will need to obtain (select) the artist name, song name, and song length from out table, and we will need to filter by sessionId and itemInSession. In CQL our query looks like:\n",
    "\n",
    "SELECT artist, song_title, song_length FROM session_songs WHERE sessionId = 338 AND itemInSession = 4\n",
    "\n",
    "- We will name our table session_songs\n",
    "- Our primary key will consist of partition key sessionId, and clustering key itemInSession so that we can filter by this attributes later on.\n",
    "- The columns of our table will be: sessionId, itemInSession, artist, song_title and song_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1188bcfd780>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS session_songs\n",
    "    (sessionId int, itemInSession int, artist text, song_title text, song_length float,\n",
    "    PRIMARY KEY(sessionId, itemInSession))\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
